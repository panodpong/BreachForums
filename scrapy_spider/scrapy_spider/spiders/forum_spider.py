import scrapy

class ForumSearchSpider(scrapy.Spider):
    name = "forum_search"
    allowed_domains = ["breachforums.st"]
    start_urls = [
        "https://breachforums.st/Forum-Cracked-Accounts?page=1"
    ]

    def parse(self, response):
        current_page = response.url.split("page=")[-1] if "page=" in response.url else "1"
        self.logger.info(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á Search Results ‡∏´‡∏ô‡πâ‡∏≤ {current_page}")

        # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        # links = response.xpath('//a/@href').getall()
        links = response.xpath('//a').extract()
        filename = f"search-page{current_page}.txt"
        with open(filename, "w", encoding="utf-8") as f:
            for link in links:
                f.write(link + "\n")

        # ‡∏´‡∏≤‡∏õ‡∏∏‡πà‡∏° Next
        next_page = response.xpath('//a[contains(@class, "pagination_next")]/@href').get()
        if next_page:
            next_page_url = response.urljoin(next_page)
            self.logger.info(f"‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤: {next_page_url}")
            yield scrapy.Request(next_page_url, callback=self.parse)
        else:
            self.logger.info("üèÅ ‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
# import scrapy

# class ForumSearchSpider(scrapy.Spider):
#     name = "forum_search"
#     allowed_domains = ["breachforums.st"]
#     start_urls = [
#         "https://breachforums.st/Forum-Databases?page=1"
#     ]

#     def parse(self, response):
#         current_page = response.url.split("page=")[-1] if "page=" in response.url else "1"
#         self.logger.info(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á Search Results ‡∏´‡∏ô‡πâ‡∏≤ {current_page}")

#         # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å <a> 
#         links = response.xpath('//a')
        
#         filename = f"search-page{current_page}.txt"
#         with open(filename, "w", encoding="utf-8") as f:
#             for link in links:
#                 # ‡∏î‡∏∂‡∏á href ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å <a>
#                 href = link.xpath('@href').get()  # ‡∏î‡∏∂‡∏á href attribute
#                 text = link.xpath('text()').get()  # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å <a>

#                 # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ href ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
#                 if href and text:
#                     f.write(f"{{{href}}}{{{text}}}\n")

#         # ‡∏´‡∏≤‡∏õ‡∏∏‡πà‡∏° Next
#         next_page = response.xpath('//a[contains(@class, "pagination_next")]/@href').get()
#         if next_page:
#             next_page_url = response.urljoin(next_page)
#             self.logger.info(f"‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤: {next_page_url}")
#             yield scrapy.Request(next_page_url, callback=self.parse)
#         else:
#             self.logger.info("üèÅ ‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
